{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Logistic Regression\n",
    "\n",
    "Build a logistic regression model to predict whether a student gets admitted into a university.\n",
    "\n",
    "<a></a>\n",
    "### Problem Statement\n",
    "\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. \n",
    "* You have historical data from previous applicants that you can use as a training set for logistic regression. \n",
    "* For each training example, you have the applicant’s scores on two exams and the admissions decision. \n",
    "* Your task is to build a classification model that estimates an applicant’s probability of admission based on the scores from those two exams. \n",
    "\n",
    "<a></a>\n",
    "### Loading and visualizing the data\n",
    "\n",
    "You will start by loading the dataset for this task. \n",
    "- The `load_dataset()` function shown below loads the data into variables `X_train` and `y_train`\n",
    "  - `X_train` contains exam scores on two exams for a student\n",
    "  - `y_train` is the admission decision \n",
    "      - `y_train = 1` if the student was admitted \n",
    "      - `y_train = 0` if the student was not admitted \n",
    "  - Both `X_train` and `y_train` are numpy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "y_train = np.array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n",
    "X_train = np.array([\n",
    "    [34.62365962451697, 78.0246928153624], [30.28671076822607, 43.89499752400101], [35.84740876993872, 72.90219802708364], [60.18259938620976, 86.30855209546826], [79.0327360507101, 75.3443764369103], [45.08327747668339, 56.3163717815305], [61.10666453684766, 96.51142588489624], [75.02474556738889, 46.55401354116538], [76.09878670226257, 87.42056971926803], [84.43281996120035, 43.53339331072109], [95.86155507093572, 38.22527805795094], [75.01365838958247, 30.60326323428011], [82.30705337399482, 76.48196330235604], [69.36458875970939, 97.71869196188608], [39.53833914367223, 76.03681085115882], [53.9710521485623, 89.20735013750205], [69.07014406283025, 52.74046973016765], [67.94685547711617, 46.67857410673128], [70.66150955499435, 92.92713789364832], [76.97878372747498, 47.57596364975532], [67.37202754570876, 42.83843832029179], [89.6767757507208, 65.79936592745237], [50.534788289883, 48.85581152764205], [34.21206097786789, 44.20952859866288], [77.9240914545704, 68.9723599933059], [62.27101367004632, 69.95445795447587], [80.1901807509566, 44.82162893218353], [93.114388797442, 38.80067033713209], [61.83020602312595, 50.25610789244621], [38.78580379679423, 64.99568095539578], [61.379289447425, 72.80788731317097], [85.40451939411645, 57.05198397627122], [52.10797973193984, 63.12762376881715], [52.04540476831827, 69.43286012045222], [40.23689373545111, 71.16774802184875], [54.63510555424817, 52.21388588061123], [33.91550010906887, 98.86943574220612], [64.17698887494485, 80.90806058670817], [74.78925295941542, 41.57341522824434], [34.1836400264419, 75.2377203360134], [83.90239366249155, 56.30804621605327], [51.54772026906181, 46.85629026349976], [94.44336776917852, 65.56892160559052], [82.36875375713919, 40.61825515970618], [51.04775177128865, 45.82270145776001], [62.22267576120188, 52.06099194836679], [77.19303492601364, 70.45820000180959], [97.77159928000232, 86.7278223300282], [62.07306379667647, 96.76882412413984], [91.56497449807442, 88.696292545466], [79.94481794066932, 74.16311935043758], [99.2725269292572, 60.99903099844988], [90.54671411399852, 43.39060180650027], [34.52451385320009, 60.39634245837173], [50.2864961189907, 49.80453881323059], [49.58667721632031, 59.80895099453265], [97.64563396007767, 68.86157272420604], [32.57720016809309, 95.59854761387876], [74.24869136721598, 69.82457122657193], [71.7964620586338, 78.45356224515052], [75.3956114656803, 85.75993667331619], [35.28611281526193, 47.02051394723416], [56.25381749711624, 39.26147251058019], [30.05882244669796, 49.59297386723685], [44.66826172480893, 66.45008614558913], [66.56089447242954, 41.09209807936973], [40.45755098375164, 97.53518548909936], [49.07256321908844, 51.88321182073966], [80.27957401466998, 92.11606081344084], [66.74671856944039, 60.99139402740988], [32.72283304060323, 43.30717306430063], [64.0393204150601, 78.03168802018232], [72.34649422579923, 96.22759296761404], [60.45788573918959, 73.09499809758037], [58.84095621726802, 75.85844831279042], [99.82785779692128, 72.36925193383885], [47.26426910848174, 88.47586499559782], [50.45815980285988, 75.80985952982456], [60.45555629271532, 42.50840943572217], [82.22666157785568, 42.71987853716458], [88.9138964166533, 69.80378889835472], [94.83450672430196, 45.69430680250754], [67.31925746917527, 66.58935317747915], [57.23870631569862, 59.51428198012956], [80.36675600171273, 90.96014789746954], [68.46852178591112, 85.59430710452014], [42.0754545384731, 78.84478600148043], [75.47770200533905, 90.42453899753964], [78.63542434898018, 96.64742716885644], [52.34800398794107, 60.76950525602592], [94.09433112516793, 77.15910509073893], [90.44855097096364, 87.50879176484702], [55.48216114069585, 35.57070347228866], [74.49269241843041, 84.84513684930135], [89.84580670720979, 45.35828361091658], [83.48916274498238, 48.38028579728175], [42.2617008099817, 87.10385094025457], [99.31500880510394, 68.77540947206617], [55.34001756003703, 64.9319380069486], [74.77589300092767, 89.52981289513276]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (100, 2)\n",
      "The shape of y_train is: (100,)\n",
      "We have m = 100 training examples\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is: ' + str(X_train.shape))\n",
    "print ('The shape of y_train is: ' + str(y_train.shape))\n",
    "print ('We have m = %d training examples' % (len(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a></a>\n",
    "### Sigmoid function\n",
    "\n",
    "For logistic regression, the model is represented as\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function\n",
    "\n",
    "def sigmoid(z):\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a></a>\n",
    "### Cost function for regularized logistic regression\n",
    "\n",
    "Implement the cost function for regularized logistic regression.\n",
    "\n",
    "For regularized logistic regression, the cost function is of the form\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Function with Regularization\n",
    "\n",
    "def compute_cost(X, y, w, b, lambda_ = 1):\n",
    "    m,n = X.shape\n",
    "\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        f_wb = sigmoid(np.dot(X[i],w) + b)\n",
    "        cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)\n",
    "    cost = cost / m\n",
    "\n",
    "    reg_cost = 0.\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j] ** 2)\n",
    "    reg_cost = reg_cost * (lambda_/(2*m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a></a>\n",
    "### Gradient for logistic regression\n",
    "\n",
    "In this section, you will implement the gradient for logistic regression.\n",
    "\n",
    "Recall that the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $b$, $w_j$ are all updated simultaniously\n",
    "\n",
    "The regularization aspect comes in by adding lambda/m * w[j] at th very end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Gradient for Regularized Logistic Regression\n",
    "\n",
    "def compute_gradient(X, y, w, b, lambda_):\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = sigmoid(np.dot(X[i],w) + b)\n",
    "        err = f_wb - y[i]\n",
    "        dj_db += err\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i,j]\n",
    "            \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent \n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "        \n",
    "    return w_in, b_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.001\n",
    "\n",
    "w,b = gradient_descent(X_train ,y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a></a>\n",
    "### Predicting\n",
    "\n",
    "The `predict` function produces `1` or `0` predictions given a dataset and a learned parameter vector $w$ and $b$.\n",
    "- First you need to compute the prediction from the model $f(x^{(i)}) = g(w \\cdot x^{(i)} + b)$ for every example \n",
    "    - You've implemented this before in the parts above\n",
    "- We interpret the output of the model ($f(x^{(i)})$) as the probability that $y^{(i)}=1$ given $x^{(i)}$ and parameterized by $w$.\n",
    "- Therefore, to get a final prediction ($y^{(i)}=0$ or $y^{(i)}=1$) from the logistic regression model, you can use the following heuristic \n",
    "\n",
    "  if $f(x^{(i)}) >= 0.5$, predict $y^{(i)}=1$\n",
    "  \n",
    "  if $f(x^{(i)}) < 0.5$, predict $y^{(i)}=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "def predict(X, w, b):\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "\n",
    "    for i in range(m):   \n",
    "        f_wb = sigmoid(np.dot(X[i],w) + b)\n",
    "        p[i] = f_wb >= 0.5\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of predict: shape (4,), value [0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Test your predict code\n",
    "np.random.seed(1)\n",
    "tmp_w = np.random.randn(2)\n",
    "tmp_b = 0.3    \n",
    "tmp_X = np.random.randn(4, 2) - 0.5\n",
    "\n",
    "tmp_p = predict(tmp_X, tmp_w, tmp_b)\n",
    "print(f'Output of predict: shape {tmp_p.shape}, value {tmp_p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 92.000000\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = predict(X_train, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
